'''
/home/gpu-agx/zoo/script/Validation_alone/validation_alone.py
ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ì¶”ë¡  ì‹œê°„ê³¼ ì •í™•ë„(accuracy, precision, recall, F1-score)ë¥¼ ì¸¡ì •í•˜ì—¬
zoo/result/Validation_alone/ì— logì™€ txt íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
ê° ëª¨ë¸ íƒ€ì…ë³„ë¡œ ì ì ˆí•œ validation ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì˜ˆì „ pod validation ìŠ¤í¬ë¦½íŠ¸ ë°©ì‹ì„ ì°¸ê³ í•˜ì—¬ predict()ë¥¼ ì‚¬ìš©í•˜ê³  ground truthì™€ ì§ì ‘ ë¹„êµí•©ë‹ˆë‹¤.

ì¤‘ìš”: ê° engineë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë™ì‹œ/ë³‘ë ¬ ì‹¤í–‰ì€ ì ˆëŒ€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
'''
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from collections import defaultdict

try:
    import torch
except ImportError:
    torch = None

from ultralytics import YOLO
import yaml


ROOT = Path(__file__).resolve().parent.parent.parent
MODEL_DIR = ROOT / "model"
DATA_DIR = ROOT / "dataset"
RESULT_DIR = ROOT / "result" / "Validation_alone_withoutval"

# coco128ì€ YOLO í˜•ì‹ labelsë¥¼ ì‚¬ìš© (txt íŒŒì¼)

MODEL_SPECS: List[Dict] = [
    {
        "name": "yolo11n-cls",
        "engine": "yolo11n-cls.engine",
        "dataset": "ImageNet1k_100",  # ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€
        "imgsz": 224,
        "task": "classify",
    },
    {
        "name": "yolo11n-detect",
        "engine": "yolo11n-detect.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "detect",
    },
    {
        "name": "yolo11n-pose",
        "engine": "yolo11n-pose.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "pose",
    },
    {
        "name": "yolo11n-seg",
        "engine": "yolo11n-seg.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "segment",
    },
    {
        "name": "yolo11n-obb",
        "engine": "yolo11n-obb.engine",
        "dataset": "DOTAv1.5-1024_val",
        "imgsz": 640,
        "task": "obb",
    },
]


# COCO 80 classes (ê³µì‹ í´ë˜ìŠ¤ ì´ë¦„)
COCO80_NAMES: List[str] = [
    "person",
    "bicycle",
    "car",
    "motorcycle",
    "airplane",
    "bus",
    "train",
    "truck",
    "boat",
    "traffic light",
    "fire hydrant",
    "stop sign",
    "parking meter",
    "bench",
    "bird",
    "cat",
    "dog",
    "horse",
    "sheep",
    "cow",
    "elephant",
    "bear",
    "zebra",
    "giraffe",
    "backpack",
    "umbrella",
    "handbag",
    "tie",
    "suitcase",
    "frisbee",
    "skis",
    "snowboard",
    "sports ball",
    "kite",
    "baseball bat",
    "baseball glove",
    "skateboard",
    "surfboard",
    "tennis racket",
    "bottle",
    "wine glass",
    "cup",
    "fork",
    "knife",
    "spoon",
    "bowl",
    "banana",
    "apple",
    "sandwich",
    "orange",
    "broccoli",
    "carrot",
    "hot dog",
    "pizza",
    "donut",
    "cake",
    "chair",
    "couch",
    "potted plant",
    "bed",
    "dining table",
    "toilet",
    "tv",
    "laptop",
    "mouse",
    "remote",
    "keyboard",
    "cell phone",
    "microwave",
    "oven",
    "toaster",
    "sink",
    "refrigerator",
    "book",
    "clock",
    "vase",
    "scissors",
    "teddy bear",
    "hair drier",
    "toothbrush",
]


def synchronize():
    """GPU ë™ê¸°í™”"""
    if torch is not None and torch.cuda.is_available():
        torch.cuda.synchronize()


def format_metrics(metrics: Dict[str, Any], task: str) -> str:
    """ë©”íŠ¸ë¦­ì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    lines = []
    
    if task == "classify":
        if "top1" in metrics:
            lines.append(f"  Top-1 Accuracy: {metrics['top1']:.4f}")
        if "top5" in metrics:
            lines.append(f"  Top-5 Accuracy: {metrics['top5']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_top1" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] Top-1: {metrics['official_top1']:.4f}")
        if "official_top5" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] Top-5: {metrics['official_top5']:.4f}")
    elif task == "detect":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] mAP@[0.5:0.95]: {metrics['official_map50_95']:.4f}")
    elif task == "segment":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_mask_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] mask mAP@[0.5:0.95]: {metrics['official_mask_map50_95']:.4f}")
    elif task == "pose":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_oks_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] OKS mAP@[0.5:0.95]: {metrics['official_oks_map50_95']:.4f}")
    elif task == "obb":
        if "total_detections" in metrics:
            lines.append(f"  Total Detections: {metrics['total_detections']}")
        if "avg_detections_per_image" in metrics:
            lines.append(f"  Avg Detections/Image: {metrics['avg_detections_per_image']:.2f}")
        # ê³µì‹ ì§€í‘œ
        if "official_oriented_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] oriented mAP@[0.5:0.95]: {metrics['official_oriented_map50_95']:.4f}")
    
    return "\n".join(lines)


def validate_classify(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """Classification validation: ImageNet1k_100 (ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€)"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: {data_path}")
    log_lines.append(f"  Note: ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€")
    log_lines.append(f"{'='*80}\n")
    
    # ImageNet1k_100 êµ¬ì¡°: val/0/image.jpg, val/1/image.jpg, ...
    val_dir = data_path / "val"
    if not val_dir.exists():
        raise FileNotFoundError(f"Validation directory not found: {val_dir}")
    
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (val í´ë” ë‚´ í´ë˜ìŠ¤ë³„ í´ë”ì—ì„œ)
    val_images = sorted([p for p in val_dir.rglob("*.jpg")] + 
                       [p for p in val_dir.rglob("*.png")])
    if not val_images:
        raise RuntimeError(f"No images found in {val_dir}")
    
    log_lines.append(f"ê²€ì¦ ì´ë¯¸ì§€: {len(val_images)}ê°œ")
    log_lines.append("Note: ì •í™•ë„ ê³„ì‚° ì—†ì´ ì¶”ë¡  ì‹œê°„ë§Œ ì¸¡ì •í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    _ = model.predict(
        source=str(val_images[0]),
        imgsz=spec["imgsz"],
        device=0,
        save=False,
        verbose=False,
    )
    synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì • (ì •í™•ë„ ê³„ì‚° ì—†ì´)
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    start_time = time.perf_counter()
    
    for img_path in val_images:
        synchronize()  # ì´ì „ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        _ = model.predict(
            source=str(img_path),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        synchronize()  # CUDA runtime ë™ê¸°í™”ë¡œ TensorRT ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    
    elapsed_time = time.perf_counter() - start_time
    num_images = len(val_images)
    avg_time_per_image = elapsed_time / num_images if num_images > 0 else 0
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {num_images}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": spec["dataset"],
        "total_time": elapsed_time,
        "num_images": num_images,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": {},  # ë©”íŠ¸ë¦­ ì—†ìŒ
    }


def validate_detect_seg_pose(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """Detection/Segmentation/Pose validation: coco2017_val ë°ì´í„°ì…‹ ì‚¬ìš© (COCO JSON í˜•ì‹)"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: COCO 2017 Val (coco2017_val)")
    log_lines.append(f"  Task: {spec['task']}")
    log_lines.append(f"{'='*80}\n")
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
    images_dir = data_path / "val2017"
    
    if not images_dir.exists():
        raise FileNotFoundError(f"Images directory not found: {images_dir}")
    
    # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ (100ê°œë¡œ ì œí•œ)
    image_files = sorted(list(images_dir.glob("*.jpg")) + 
                        list(images_dir.glob("*.png")))[:100]
    
    log_lines.append(f"ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘: {len(image_files)}ê°œ (ìµœëŒ€ 100ê°œë¡œ ì œí•œ)")
    log_lines.append("Note: ì •í™•ë„ ê³„ì‚° ì—†ì´ ì¶”ë¡  ì‹œê°„ë§Œ ì¸¡ì •í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    if image_files:
        _ = model.predict(
            source=str(image_files[0]),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì • (ì •í™•ë„ ê³„ì‚° ì—†ì´)
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    start_time = time.perf_counter()
    
    # ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
    for img_file in image_files:
        synchronize()  # ì´ì „ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        _ = model.predict(
            source=str(img_file),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        synchronize()  # CUDA runtime ë™ê¸°í™”ë¡œ TensorRT ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    
    elapsed_time = time.perf_counter() - start_time
    num_images = len(image_files)
    avg_time_per_image = elapsed_time / num_images if num_images > 0 else 0
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {num_images}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": "coco2017_val",
        "total_time": elapsed_time,
        "num_images": num_images,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": {},  # ë©”íŠ¸ë¦­ ì—†ìŒ
    }


def validate_obb(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """OBB validation: DOTAv1.5-1024_val"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    val_images_dir = data_path / "images" / "val"
    val_labels_dir = data_path / "labels" / "val"
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: DOTA v1.5-1024 Val")
    log_lines.append(f"  Images: {val_images_dir}")
    log_lines.append(f"{'='*80}\n")
    
    if not val_images_dir.exists():
        raise FileNotFoundError(f"Images directory not found: {val_images_dir}")
    
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (100ê°œë¡œ ì œí•œ)
    val_images = sorted([f for f in val_images_dir.iterdir()
                        if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])[:100]

    # train í´ë”ë„ í™•ì¸ (í•„ìš”ì‹œ ì‚¬ìš©)
    train_images_dir = data_path / "images" / "train"
    train_images = []
    if train_images_dir.exists():
        train_images = sorted([f for f in train_images_dir.iterdir()
                              if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])

    # val ì´ë¯¸ì§€ë¥¼ ìš°ì„  ì‚¬ìš©, ë¶€ì¡±í•˜ë©´ trainì—ì„œ ë³´ì¶© (ìµœëŒ€ 100ê°œ)
    combined_images = val_images
    if len(combined_images) < 100 and train_images:
        needed = 100 - len(combined_images)
        combined_images = combined_images + train_images[:needed]
    
    # ìµœì¢…ì ìœ¼ë¡œ 100ê°œë¡œ ì œí•œ
    combined_images = combined_images[:100]

    if not combined_images:
        raise RuntimeError(f"No images found in {val_images_dir}")

    log_lines.append(
        f"ê²€ì¦ ì´ë¯¸ì§€: {len(combined_images)}ê°œ (ìµœëŒ€ 100ê°œë¡œ ì œí•œ) "
        f"(val: {min(len(val_images), 100)}ê°œ, train ì‚¬ìš©: {max(0, len(combined_images)-len(val_images))}ê°œ)"
    )
    log_lines.append("Note: ì •í™•ë„ ê³„ì‚° ì—†ì´ ì¶”ë¡  ì‹œê°„ë§Œ ì¸¡ì •í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    _ = model.predict(
        source=str(val_images[0]),
        imgsz=spec["imgsz"],
        device=0,
        save=False,
        verbose=False,
    )
    synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì • (ì •í™•ë„ ê³„ì‚° ì—†ì´)
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    start_time = time.perf_counter()
    
    for img_path in combined_images:
        synchronize()  # ì´ì „ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        _ = model.predict(
            source=str(img_path),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        synchronize()  # CUDA runtime ë™ê¸°í™”ë¡œ TensorRT ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    
    elapsed_time = time.perf_counter() - start_time
    total_processed = len(combined_images)
    avg_time_per_image = elapsed_time / total_processed if total_processed > 0 else 0
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {total_processed}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": "DOTAv1.5-1024_val",
        "total_time": elapsed_time,
        "num_images": total_processed,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": {},  # ë©”íŠ¸ë¦­ ì—†ìŒ
    }


def validate_model(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """
    ëª¨ë¸ validation ìˆ˜í–‰ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    
    ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ì•¼ í•˜ë©°, ë³‘ë ¬ ì‹¤í–‰ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
    ê° ëª¨ë¸ì˜ validationì´ ì™„ì „íˆ ëë‚œ í›„ì—ë§Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    if spec["task"] == "classify":
        return validate_classify(spec, log_lines)
    elif spec["task"] == "obb":
        return validate_obb(spec, log_lines)
    else:
        # detect, pose, segment
        return validate_detect_seg_pose(spec, log_lines)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    RESULT_DIR.mkdir(parents=True, exist_ok=True)
    log_path = RESULT_DIR / "validation_alone.log"
    txt_path = RESULT_DIR / "validation_alone.txt"
    json_path = RESULT_DIR / "validation_alone.json"
    
    log_lines: List[str] = []
    summary_lines: List[str] = []
    summary_data: List[Dict] = []
    
    log_lines.append("="*80)
    log_lines.append("Validation ì‹œì‘")
    log_lines.append(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {RESULT_DIR}")
    log_lines.append("âš ï¸  ì¤‘ìš”: ê° engineì€ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë³‘ë ¬ ì‹¤í–‰ ì—†ìŒ)")
    log_lines.append("="*80)
    log_lines.append("")
    
    # ê° engineë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ validation ìˆ˜í–‰ (ë³‘ë ¬ ì‹¤í–‰ ì ˆëŒ€ ê¸ˆì§€)
    for idx, spec in enumerate(MODEL_SPECS, 1):
        try:
            print(f"\n[{idx}/{len(MODEL_SPECS)}] {spec['name']} validation ì‹œì‘ (ìˆœì°¨ ì‹¤í–‰)")
            summary = validate_model(spec, log_lines)
            summary_data.append(summary)
            print(f"[{idx}/{len(MODEL_SPECS)}] {spec['name']} validation ì™„ë£Œ")
            
            # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
            summary_lines.append(f"\n{'='*80}")
            summary_lines.append(f"[{summary['model']}]")
            summary_lines.append(f"  Dataset: {summary['dataset']}")
            summary_lines.append(f"  Task: {summary['task']}")
            summary_lines.append(f"  ì „ì²´ ì‹œê°„: {summary['total_time']:.3f}ì´ˆ")
            if summary['num_images'] > 0:
                summary_lines.append(f"  ì´ë¯¸ì§€ ìˆ˜: {summary['num_images']}")
                summary_lines.append(f"  í‰ê·  ì‹œê°„: {summary['avg_time_per_image_ms']:.3f}ms")
            
            # ë©”íŠ¸ë¦­ ì¶”ê°€ (ì—†ìœ¼ë©´ ìƒëµ)
            if summary.get('metrics'):
                metrics_str = format_metrics(summary['metrics'], summary['task'])
                if metrics_str:
                    summary_lines.append("  ë©”íŠ¸ë¦­:")
                    for line in metrics_str.split("\n"):
                        if line.strip():
                            summary_lines.append(line)
            
        except Exception as e:
            error_msg = f"[{spec['name']}] ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            log_lines.append(error_msg)
            log_lines.append("")
            summary_lines.append(error_msg)
            print(f"âŒ {error_msg}")
            import traceback
            log_lines.append(traceback.format_exc())
            log_lines.append("")
    
    # ê²°ê³¼ ì €ì¥
    log_lines.append("\n" + "="*80)
    log_lines.append("Validation ì™„ë£Œ")
    log_lines.append("="*80)
    
    log_path.write_text("\n".join(log_lines), encoding="utf-8")
    txt_path.write_text("\n".join(summary_lines), encoding="utf-8")
    
    # JSON í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (êµ¬ì¡°í™”ëœ ë°ì´í„°)
    json_path.write_text(
        json.dumps(summary_data, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )
    
    print(f"\nâœ… Validation ì™„ë£Œ!")
    print(f"ğŸ“„ ë¡œê·¸ íŒŒì¼: {log_path}")
    print(f"ğŸ“„ ìš”ì•½ íŒŒì¼: {txt_path}")
    print(f"ğŸ“„ JSON íŒŒì¼: {json_path}")


if __name__ == "__main__":
    main()
