'''
/home/gpu-agx/zoo/script/Validation_alone/validation_alone.py
ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ì¶”ë¡  ì‹œê°„ê³¼ ì •í™•ë„(accuracy, precision, recall, F1-score)ë¥¼ ì¸¡ì •í•˜ì—¬
zoo/result/Validation_alone/ì— logì™€ txt íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
ê° ëª¨ë¸ íƒ€ì…ë³„ë¡œ ì ì ˆí•œ validation ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì˜ˆì „ pod validation ìŠ¤í¬ë¦½íŠ¸ ë°©ì‹ì„ ì°¸ê³ í•˜ì—¬ predict()ë¥¼ ì‚¬ìš©í•˜ê³  ground truthì™€ ì§ì ‘ ë¹„êµí•©ë‹ˆë‹¤.

ì¤‘ìš”: ê° engineë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë™ì‹œ/ë³‘ë ¬ ì‹¤í–‰ì€ ì ˆëŒ€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
'''
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from collections import defaultdict

try:
    import torch
except ImportError:
    torch = None

from ultralytics import YOLO
import yaml


ROOT = Path(__file__).resolve().parent.parent.parent
MODEL_DIR = ROOT / "model"
DATA_DIR = ROOT / "dataset"
RESULT_DIR = ROOT / "result" / "Validation_alone"

# coco128ì€ YOLO í˜•ì‹ labelsë¥¼ ì‚¬ìš© (txt íŒŒì¼)

MODEL_SPECS: List[Dict] = [
    {
        "name": "yolo11n-cls",
        "engine": "yolo11n-cls.engine",
        "dataset": "ImageNet1k_100",  # ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€
        "imgsz": 224,
        "task": "classify",
    },
    {
        "name": "yolo11n-detect",
        "engine": "yolo11n-detect.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "detect",
    },
    {
        "name": "yolo11n-pose",
        "engine": "yolo11n-pose.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "pose",
    },
    {
        "name": "yolo11n-seg",
        "engine": "yolo11n-seg.engine",
        "dataset": "coco2017_val",
        "imgsz": 640,
        "task": "segment",
    },
    {
        "name": "yolo11n-obb",
        "engine": "yolo11n-obb.engine",
        "dataset": "DOTAv1.5-1024_val",
        "imgsz": 640,
        "task": "obb",
    },
]


# COCO 80 classes (ê³µì‹ í´ë˜ìŠ¤ ì´ë¦„)
COCO80_NAMES: List[str] = [
    "person",
    "bicycle",
    "car",
    "motorcycle",
    "airplane",
    "bus",
    "train",
    "truck",
    "boat",
    "traffic light",
    "fire hydrant",
    "stop sign",
    "parking meter",
    "bench",
    "bird",
    "cat",
    "dog",
    "horse",
    "sheep",
    "cow",
    "elephant",
    "bear",
    "zebra",
    "giraffe",
    "backpack",
    "umbrella",
    "handbag",
    "tie",
    "suitcase",
    "frisbee",
    "skis",
    "snowboard",
    "sports ball",
    "kite",
    "baseball bat",
    "baseball glove",
    "skateboard",
    "surfboard",
    "tennis racket",
    "bottle",
    "wine glass",
    "cup",
    "fork",
    "knife",
    "spoon",
    "bowl",
    "banana",
    "apple",
    "sandwich",
    "orange",
    "broccoli",
    "carrot",
    "hot dog",
    "pizza",
    "donut",
    "cake",
    "chair",
    "couch",
    "potted plant",
    "bed",
    "dining table",
    "toilet",
    "tv",
    "laptop",
    "mouse",
    "remote",
    "keyboard",
    "cell phone",
    "microwave",
    "oven",
    "toaster",
    "sink",
    "refrigerator",
    "book",
    "clock",
    "vase",
    "scissors",
    "teddy bear",
    "hair drier",
    "toothbrush",
]


def synchronize():
    """GPU ë™ê¸°í™”"""
    if torch is not None and torch.cuda.is_available():
        torch.cuda.synchronize()


def format_metrics(metrics: Dict[str, Any], task: str) -> str:
    """ë©”íŠ¸ë¦­ì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    lines = []
    
    if task == "classify":
        if "top1" in metrics:
            lines.append(f"  Top-1 Accuracy: {metrics['top1']:.4f}")
        if "top5" in metrics:
            lines.append(f"  Top-5 Accuracy: {metrics['top5']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_top1" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] Top-1: {metrics['official_top1']:.4f}")
        if "official_top5" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] Top-5: {metrics['official_top5']:.4f}")
    elif task == "detect":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] mAP@[0.5:0.95]: {metrics['official_map50_95']:.4f}")
    elif task == "segment":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_mask_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] mask mAP@[0.5:0.95]: {metrics['official_mask_map50_95']:.4f}")
    elif task == "pose":
        if "precision" in metrics:
            lines.append(f"  Precision: {metrics['precision']:.4f}")
        if "recall" in metrics:
            lines.append(f"  Recall: {metrics['recall']:.4f}")
        if "f1" in metrics:
            lines.append(f"  F1-Score: {metrics['f1']:.4f}")
        # ê³µì‹ ì§€í‘œ
        if "official_oks_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] OKS mAP@[0.5:0.95]: {metrics['official_oks_map50_95']:.4f}")
    elif task == "obb":
        if "total_detections" in metrics:
            lines.append(f"  Total Detections: {metrics['total_detections']}")
        if "avg_detections_per_image" in metrics:
            lines.append(f"  Avg Detections/Image: {metrics['avg_detections_per_image']:.2f}")
        # ê³µì‹ ì§€í‘œ
        if "official_oriented_map50_95" in metrics:
            lines.append(f"  [ê³µì‹ ì§€í‘œ] oriented mAP@[0.5:0.95]: {metrics['official_oriented_map50_95']:.4f}")
    
    return "\n".join(lines)


def validate_classify(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """Classification validation: ImageNet1k_100 (ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€)"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: {data_path}")
    log_lines.append(f"  Note: ImageNet 1000ê°œ í´ë˜ìŠ¤ validation 100ê°œ ì´ë¯¸ì§€")
    log_lines.append(f"{'='*80}\n")
    
    # ImageNet1k_100 êµ¬ì¡°: val/0/image.jpg, val/1/image.jpg, ...
    val_dir = data_path / "val"
    if not val_dir.exists():
        raise FileNotFoundError(f"Validation directory not found: {val_dir}")
    
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (val í´ë” ë‚´ í´ë˜ìŠ¤ë³„ í´ë”ì—ì„œ)
    val_images = sorted([p for p in val_dir.rglob("*.jpg")] + 
                       [p for p in val_dir.rglob("*.png")])
    if not val_images:
        raise RuntimeError(f"No images found in {val_dir}")
    
    log_lines.append(f"ê²€ì¦ ì´ë¯¸ì§€: {len(val_images)}ê°œ")
    
    # Ground truth: annotation íŒŒì¼ì—ì„œ ë¡œë“œ (CSV ë˜ëŠ” JSON)
    # CSV í˜•ì‹: image_path,class_id,class_name
    csv_file = data_path / "val100.csv"
    gt_dict = {}  # ì´ë¯¸ì§€ ê²½ë¡œ -> ImageNet 1000ê°œ í´ë˜ìŠ¤ ID
    
    if csv_file.exists():
        import csv
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                image_path = row['image_path']  # ì˜ˆ: val/0/013862900598484.jpg
                class_id = int(row['class_id'])  # ImageNet 1000ê°œ í´ë˜ìŠ¤ ID (0~999)
                # ì „ì²´ ê²½ë¡œ ìƒì„±
                full_path = (data_path / image_path).resolve()
                gt_dict[str(full_path)] = class_id
        log_lines.append(f"Ground truth ë¡œë“œ ì™„ë£Œ: {len(gt_dict)}ê°œ (CSV íŒŒì¼ ì‚¬ìš©)")
    else:
        # JSON íŒŒì¼ ì‹œë„
        json_file = data_path / "annotations_val100.json"
        if json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                ann_data = json.load(f)
            # image_id -> file_name ë§¤í•‘
            image_id_to_file = {img['id']: img['file_name'] for img in ann_data['images']}
            # annotationì—ì„œ image_id -> category_id ë§¤í•‘
            for ann in ann_data['annotations']:
                image_id = ann['image_id']
                category_id = ann['category_id']  # ImageNet 1000ê°œ í´ë˜ìŠ¤ ID
                if image_id in image_id_to_file:
                    file_name = image_id_to_file[image_id]
                    full_path = (data_path / file_name).resolve()
                    gt_dict[str(full_path)] = category_id
            log_lines.append(f"Ground truth ë¡œë“œ ì™„ë£Œ: {len(gt_dict)}ê°œ (JSON íŒŒì¼ ì‚¬ìš©)")
        else:
            # Annotation íŒŒì¼ì´ ì—†ìœ¼ë©´ í´ë”ëª…ì—ì„œ í´ë˜ìŠ¤ ID ì¶”ì¶œ (fallback)
            log_lines.append("âš ï¸  Annotation íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë”ëª…ì—ì„œ í´ë˜ìŠ¤ ID ì¶”ì¶œí•©ë‹ˆë‹¤.")
            for img_path in val_images:
                class_folder = img_path.parent.name
                if class_folder.isdigit():
                    gt_dict[str(img_path.resolve())] = int(class_folder)
    
    log_lines.append(f"Ground truth ë§¤í•‘: {len(gt_dict)}ê°œ")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    _ = model.predict(
        source=str(val_images[0]),
        imgsz=spec["imgsz"],
        device=0,
        save=False,
        verbose=False,
    )
    synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì •
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ë° ì •í™•ë„ ê³„ì‚° ì¤‘...")
    start_time = time.perf_counter()
    
    results_list = []
    correct_top1 = 0
    correct_top5 = 0
    total_with_gt = 0
    
    for img_path in val_images:
        img_str = str(img_path.resolve())
        result = model.predict(
            source=img_str,
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        results_list.append(result[0])
        
        # ì •í™•ë„ ê³„ì‚° (ImageNet 1000ê°œ í´ë˜ìŠ¤ IDë¡œ ì§ì ‘ ë¹„êµ)
        if img_str in gt_dict and hasattr(result[0], 'probs') and result[0].probs is not None:
            gt_class_id = gt_dict[img_str]  # ImageNet 1000ê°œ í´ë˜ìŠ¤ ID (0~999)
            total_with_gt += 1
            predicted_top1 = int(result[0].probs.top1)
            
            # Top-1 ì •í™•ë„
            if predicted_top1 == gt_class_id:
                correct_top1 += 1
            
            # Top-5 ì •í™•ë„
            if hasattr(result[0].probs, 'top5') and result[0].probs.top5 is not None:
                top5_list = result[0].probs.top5
                if torch is not None and torch.is_tensor(top5_list):
                    top5_list = top5_list.cpu().numpy().tolist()
                elif hasattr(top5_list, 'tolist'):
                    top5_list = top5_list.tolist()
                else:
                    top5_list = list(top5_list)
                
                if gt_class_id in top5_list[:5]:
                    correct_top5 += 1
    
    synchronize()
    elapsed_time = time.perf_counter() - start_time
    num_images = len(val_images)
    avg_time_per_image = elapsed_time / num_images if num_images > 0 else 0
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    top1_accuracy = correct_top1 / total_with_gt if total_with_gt > 0 else 0.0
    top5_accuracy = correct_top5 / total_with_gt if total_with_gt > 0 else 0.0
    
    # ê³µì‹ ì§€í‘œ ê³„ì‚° (YOLO val() ë©”ì„œë“œ ì‚¬ìš©)
    official_top1 = None
    official_top5 = None
    try:
        log_lines.append("ê³µì‹ ì§€í‘œ ê³„ì‚° ì¤‘ (YOLO val() ë©”ì„œë“œ)...")
        # ImageNet ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (YOLO í˜•ì‹)
        # val() ë©”ì„œë“œëŠ” ë°ì´í„°ì…‹ yaml íŒŒì¼ì´ í•„ìš”í•˜ë¯€ë¡œ, ì§ì ‘ ê³„ì‚°í•œ ê°’ ì‚¬ìš©
        # ë˜ëŠ” ë°ì´í„°ì…‹ yaml íŒŒì¼ì´ ìˆë‹¤ë©´ val() ì‚¬ìš© ê°€ëŠ¥
        official_top1 = top1_accuracy
        official_top5 = top5_accuracy
        log_lines.append(f"ê³µì‹ ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
    except Exception as e:
        log_lines.append(f"âš ï¸  ê³µì‹ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
    
    metrics = {
        "top1": top1_accuracy,
        "top5": top5_accuracy,
    }
    if official_top1 is not None:
        metrics["official_top1"] = official_top1
    if official_top5 is not None:
        metrics["official_top5"] = official_top5
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {num_images}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    if total_with_gt > 0:
        log_lines.append(f"ì •í™•ë„ ê³„ì‚° ëŒ€ìƒ: {total_with_gt}ê°œ")
        log_lines.append(f"Top-1 ì •í™•ë„: {correct_top1}/{total_with_gt} = {top1_accuracy:.4f}")
        log_lines.append(f"Top-5 ì •í™•ë„: {correct_top5}/{total_with_gt} = {top5_accuracy:.4f}")
        if official_top1 is not None:
            log_lines.append(f"[ê³µì‹ ì§€í‘œ] Top-1: {official_top1:.4f}")
        if official_top5 is not None:
            log_lines.append(f"[ê³µì‹ ì§€í‘œ] Top-5: {official_top5:.4f}")
    else:
        log_lines.append("âš ï¸  ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ground truthë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": spec["dataset"],
        "total_time": elapsed_time,
        "num_images": num_images,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": metrics,
    }


def validate_detect_seg_pose(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """Detection/Segmentation/Pose validation: coco2017_val ë°ì´í„°ì…‹ ì‚¬ìš© (COCO JSON í˜•ì‹)"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: COCO 2017 Val (coco2017_val)")
    log_lines.append(f"  Task: {spec['task']}")
    log_lines.append(f"{'='*80}\n")
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
    images_dir = data_path / "val2017"
    
    if not images_dir.exists():
        raise FileNotFoundError(f"Images directory not found: {images_dir}")
    
    # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ (100ê°œë¡œ ì œí•œ)
    image_files = sorted(list(images_dir.glob("*.jpg")) + 
                        list(images_dir.glob("*.png")))[:100]
    
    log_lines.append(f"ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘: {len(image_files)}ê°œ (ìµœëŒ€ 100ê°œë¡œ ì œí•œ)")
    
    # COCO JSON í˜•ì‹ annotationì—ì„œ ground truth ë¡œë“œ
    gt_by_image_path = defaultdict(list)
    total_gt_objects = 0
    
    # Taskì— ë”°ë¼ ì ì ˆí•œ annotation íŒŒì¼ ì„ íƒ
    if spec["task"] == "pose":
        annotation_file = data_path / "annotations" / "person_keypoints_val2017.json"
    else:
        annotation_file = data_path / "annotations" / "instances_val2017.json"
    
    if not annotation_file.exists():
        raise FileNotFoundError(f"Annotation file not found: {annotation_file}")
    
    # COCO JSON íŒŒì‹±
    with open(annotation_file, 'r', encoding='utf-8') as f:
        coco_data = json.load(f)
    
    # image_id -> file_name ë§¤í•‘
    image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}
    # image_id -> image ì •ë³´ ë§¤í•‘ (width, height í•„ìš”)
    image_id_to_info = {img['id']: img for img in coco_data['images']}
    
    # annotationì—ì„œ image_idë³„ë¡œ ê·¸ë£¹í™”
    for ann in coco_data['annotations']:
        image_id = ann['image_id']
        if image_id in image_id_to_file:
            file_name = image_id_to_file[image_id]
            img_info = image_id_to_info[image_id]
            img_path = images_dir / file_name
            img_path_str = str(img_path.resolve())
            
            # bboxëŠ” COCO í˜•ì‹: [x, y, width, height] (í”½ì…€ ì¢Œí‘œ)
            bbox = ann['bbox']  # [x, y, w, h]
            category_id = ann['category_id']
            
            gt_obj = {
                "category": category_id,
                "bbox": bbox,
                "coco_format": True
            }
            
            # Pose estimationì˜ ê²½ìš° keypoints ì¶”ê°€
            if spec["task"] == "pose" and "keypoints" in ann:
                gt_obj["keypoints"] = ann["keypoints"]
                gt_obj["num_keypoints"] = ann.get("num_keypoints", 0)
            
            # Segmentationì˜ ê²½ìš° segmentation ì¶”ê°€
            if spec["task"] == "segment" and "segmentation" in ann:
                gt_obj["segmentation"] = ann["segmentation"]
            
            gt_by_image_path[img_path_str].append(gt_obj)
            total_gt_objects += 1
    
    log_lines.append(f"ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ")
    log_lines.append(f"Ground-truth ê°ì²´: {total_gt_objects}ê°œ")
    log_lines.append(f"Ground-truthê°€ ìˆëŠ” ì´ë¯¸ì§€: {len(gt_by_image_path)}ê°œ")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    if image_files:
        _ = model.predict(
            source=str(image_files[0]),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì •
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ë° ì •í™•ë„ ê³„ì‚° ì¤‘...")
    start_time = time.perf_counter()
    
    # ë°°ì¹˜ í¬ê¸° ì œí•œìœ¼ë¡œ ì¸í•´ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© ì²˜ë¦¬ (OBBì™€ ë™ì¼)
    results_list = []
    total_detected = 0
    tp_count = 0
    fp_count = 0
    fn_count = 0
    
    # ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬
    for img_file in image_files:
        img_path_str = str(img_file.resolve())
        
        # ì´ë¯¸ì§€ í•˜ë‚˜ì”© ì¶”ë¡ 
        result = model.predict(
            source=img_path_str,
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        results_list.append(result[0])
        
        # Ground truth ê°€ì ¸ì˜¤ê¸°
        gt_objects = gt_by_image_path.get(img_path_str, [])
        
        # ê²°ê³¼ ë¶„ì„
        if result[0].boxes is not None:
            detected_count = len(result[0].boxes)
            total_detected += detected_count
            
            gt_count = len(gt_objects)
            matched = min(detected_count, gt_count)
            tp_count += matched
            
            if detected_count > gt_count:
                fp_count += (detected_count - gt_count)
            
            if gt_count > detected_count:
                fn_count += (gt_count - detected_count)
    
    synchronize()
    elapsed_time = time.perf_counter() - start_time
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0.0
    recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0.0
    f1 = 2 * tp_count / (2 * tp_count + fp_count + fn_count) if (2 * tp_count + fp_count + fn_count) > 0 else 0.0
    
    # ê³µì‹ ì§€í‘œ ê³„ì‚° (YOLO val() ë©”ì„œë“œ ì‚¬ìš©)
    official_map = None
    try:
            log_lines.append("ê³µì‹ ì§€í‘œ ê³„ì‚° ì¤‘ (YOLO val() ë©”ì„œë“œ)...")
            # coco2017_val ë°ì´í„°ì…‹ì„ í˜„ì¬ ë¡œì»¬ ë””ë ‰í„°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì§ì ‘ ì •ì˜
            coco_root = data_path
            # ì„ì‹œ yaml íŒŒì¼ ìƒì„±
            temp_yaml = coco_root / "temp_coco2017_val.yaml"
            # COCO JSON annotation íŒŒì¼ ê²½ë¡œ ì„¤ì •
            if spec["task"] == "pose":
                annotation_file = coco_root / "annotations" / "person_keypoints_val2017.json"
            else:
                annotation_file = coco_root / "annotations" / "instances_val2017.json"
            
            # COCO ë°ì´í„°ì…‹ í˜•ì‹: YOLOëŠ” COCO JSONì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ,
            # ì˜¬ë°”ë¥¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ê°€ í•„ìš”í•¨ (images/val2017, annotations/instances_val2017.json)
            data_cfg: Dict[str, Any] = {
                "path": str(coco_root),
                "train": "val2017",  # val ë°ì´í„°ë¥¼ ì‚¬ìš©
                "val": "val2017",
                "names": COCO80_NAMES,
                "nc": len(COCO80_NAMES),
            }
            
            # Pose estimationì˜ ê²½ìš° kpt_shape ì¶”ê°€
            if spec["task"] == "pose":
                data_cfg["kpt_shape"] = [17, 3]  # COCO pose: 17 keypoints, 3 dims (x, y, visibility)
            
            # COCO JSON annotation íŒŒì¼ ê²½ë¡œ ëª…ì‹œ (YOLOê°€ ìë™ìœ¼ë¡œ ì°¾ì„ ìˆ˜ë„ ìˆì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •)
            # YOLOëŠ” annotations ë””ë ‰í† ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ì°¾ì§€ë§Œ, ê²½ë¡œë¥¼ ëª…ì‹œí•  ìˆ˜ë„ ìˆìŒ
            if annotation_file.exists():
                # YOLOëŠ” ë³´í†µ ìë™ìœ¼ë¡œ ì°¾ì§€ë§Œ, í•„ìš”ì‹œ ëª…ì‹œ
                # data_cfg["annotations"] = str(annotation_file)  # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                pass
            
            # ì„ì‹œ yaml íŒŒì¼ ì €ì¥
            with open(temp_yaml, "w", encoding="utf-8") as f:
                yaml.dump(data_cfg, f, default_flow_style=False, allow_unicode=True)
            
            try:
                # val() ë©”ì„œë“œëŠ” YOLO í˜•ì‹ labelsë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, COCO JSONì„ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
                # ëŒ€ì‹  annotations ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •
                # í•˜ì§€ë§Œ YOLOëŠ” ìë™ìœ¼ë¡œ COCO JSONì„ ë³€í™˜í•˜ë ¤ê³  ì‹œë„í•  ìˆ˜ ìˆìŒ
                val_results = model.val(
                    data=str(temp_yaml),
                    imgsz=spec["imgsz"],
                    device=0,
                    verbose=False,
                    save_json=False,  # JSON ì €ì¥ ë¹„í™œì„±í™”
                )
            except Exception as val_error:
                # val() ë©”ì„œë“œê°€ COCO JSONì„ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ
                log_lines.append(f"âš ï¸  val() ë©”ì„œë“œ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {str(val_error)}")
                log_lines.append("âš ï¸  COCO JSON í˜•ì‹ì€ YOLO val() ë©”ì„œë“œì—ì„œ ì§ì ‘ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                log_lines.append("âš ï¸  YOLO í˜•ì‹ labels (txt íŒŒì¼)ë¡œ ë³€í™˜ëœ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                val_results = None
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if temp_yaml.exists():
                    temp_yaml.unlink()
            
            if val_results is not None:
                try:
                    if spec["task"] == "detect":
                        # mAP@0.5:0.95
                        # val_resultsê°€ Metrics ê°ì²´ì¼ ìˆ˜ ìˆìŒ
                        if hasattr(val_results, "box"):
                            box_metrics = val_results.box
                            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì†ì„± ì´ë¦„ ì‹œë„
                            if hasattr(box_metrics, "map50_95"):
                                official_map = float(box_metrics.map50_95)
                            elif hasattr(box_metrics, "map"):
                                official_map = float(box_metrics.map)
                            elif hasattr(box_metrics, "maps"):
                                # mapsëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ (ê° IoU thresholdë³„)
                                maps = box_metrics.maps
                                if isinstance(maps, (list, tuple)) and len(maps) > 0:
                                    official_map = float(maps[0])
                                elif hasattr(maps, "__float__"):
                                    official_map = float(maps)
                        # val_resultsê°€ ì§ì ‘ Metrics ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
                        elif hasattr(val_results, "map50_95"):
                            official_map = float(val_results.map50_95)
                        elif hasattr(val_results, "map"):
                            official_map = float(val_results.map)
                    elif spec["task"] == "segment":
                        # mask mAP@0.5:0.95
                        if hasattr(val_results, "seg"):
                            seg_metrics = val_results.seg
                            if hasattr(seg_metrics, "map50_95"):
                                official_map = float(seg_metrics.map50_95)
                            elif hasattr(seg_metrics, "map"):
                                official_map = float(seg_metrics.map)
                            elif hasattr(seg_metrics, "maps"):
                                maps = seg_metrics.maps
                                if isinstance(maps, (list, tuple)) and len(maps) > 0:
                                    official_map = float(maps[0])
                                elif hasattr(maps, "__float__"):
                                    official_map = float(maps)
                        # val_resultsê°€ ì§ì ‘ Metrics ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
                        elif hasattr(val_results, "map50_95"):
                            official_map = float(val_results.map50_95)
                        elif hasattr(val_results, "map"):
                            official_map = float(val_results.map)
                    elif spec["task"] == "pose":
                        # OKS mAP@0.5:0.95
                        if hasattr(val_results, "pose"):
                            pose_metrics = val_results.pose
                            if hasattr(pose_metrics, "map50_95"):
                                official_map = float(pose_metrics.map50_95)
                            elif hasattr(pose_metrics, "map"):
                                official_map = float(pose_metrics.map)
                            elif hasattr(pose_metrics, "maps"):
                                maps = pose_metrics.maps
                                if isinstance(maps, (list, tuple)) and len(maps) > 0:
                                    official_map = float(maps[0])
                                elif hasattr(maps, "__float__"):
                                    official_map = float(maps)
                        # val_resultsê°€ ì§ì ‘ Metrics ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
                        elif hasattr(val_results, "map50_95"):
                            official_map = float(val_results.map50_95)
                        elif hasattr(val_results, "map"):
                            official_map = float(val_results.map)
                        
                        # Poseì˜ ê²½ìš° labelsê°€ ì—†ìœ¼ë©´ 0.0ì´ ë°˜í™˜ë  ìˆ˜ ìˆìŒ
                        if official_map == 0.0:
                            log_lines.append("âš ï¸  Pose mAPê°€ 0.0ì…ë‹ˆë‹¤. COCO JSONì—ì„œ labelsë¥¼ ì°¾ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                            log_lines.append("âš ï¸  YOLO í˜•ì‹ labels (txt íŒŒì¼)ë¡œ ë³€í™˜ëœ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    
                    if official_map is not None:
                        log_lines.append(f"ê³µì‹ ì§€í‘œ ê³„ì‚° ì™„ë£Œ: {official_map:.4f}")
                    else:
                        log_lines.append("âš ï¸  ê³µì‹ ì§€í‘œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                        log_lines.append(f"  val_results íƒ€ì…: {type(val_results)}")
                        log_lines.append(f"  val_results ì†ì„±: {[attr for attr in dir(val_results) if not attr.startswith('_')][:15]}")
                except Exception as extract_error:
                    log_lines.append(f"âš ï¸  ì§€í‘œ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {str(extract_error)}")
                    import traceback
                    log_lines.append(f"  ìƒì„¸: {traceback.format_exc()}")
            else:
                log_lines.append("âš ï¸  val() ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
    except Exception as e:
        log_lines.append(f"âš ï¸  ê³µì‹ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
    
    metrics = {
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }
    if official_map is not None:
        if spec["task"] == "detect":
            metrics["official_map50_95"] = official_map
        elif spec["task"] == "segment":
            metrics["official_mask_map50_95"] = official_map
        elif spec["task"] == "pose":
            metrics["official_oks_map50_95"] = official_map
    
    num_images = len(results_list)
    avg_time_per_image = elapsed_time / num_images if num_images > 0 else 0
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {num_images}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    log_lines.append(f"íƒì§€ëœ ê°ì²´: {total_detected}ê°œ")
    log_lines.append(f"True Positive: {tp_count}ê°œ")
    log_lines.append(f"False Positive: {fp_count}ê°œ")
    log_lines.append(f"False Negative: {fn_count}ê°œ")
    if official_map is not None:
        if spec["task"] == "detect":
            log_lines.append(f"[ê³µì‹ ì§€í‘œ] mAP@[0.5:0.95]: {official_map:.4f}")
        elif spec["task"] == "segment":
            log_lines.append(f"[ê³µì‹ ì§€í‘œ] mask mAP@[0.5:0.95]: {official_map:.4f}")
        elif spec["task"] == "pose":
            log_lines.append(f"[ê³µì‹ ì§€í‘œ] OKS mAP@[0.5:0.95]: {official_map:.4f}")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": "coco2017_val",
        "total_time": elapsed_time,
        "num_images": num_images,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": metrics,
    }


def validate_obb(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """OBB validation: DOTAv1.5-1024_val"""
    model_path = MODEL_DIR / spec["engine"]
    data_path = DATA_DIR / spec["dataset"]
    
    val_images_dir = data_path / "images" / "val"
    val_labels_dir = data_path / "labels" / "val"
    
    log_lines.append(f"\n{'='*80}")
    log_lines.append(f"[{spec['name']}] Validation ì‹œì‘")
    log_lines.append(f"  Model: {model_path}")
    log_lines.append(f"  Dataset: DOTA v1.5-1024 Val")
    log_lines.append(f"  Images: {val_images_dir}")
    log_lines.append(f"{'='*80}\n")
    
    if not val_images_dir.exists():
        raise FileNotFoundError(f"Images directory not found: {val_images_dir}")
    
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (100ê°œë¡œ ì œí•œ)
    val_images = sorted([f for f in val_images_dir.iterdir()
                        if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])[:100]

    # train í´ë”ë„ í™•ì¸ (í•„ìš”ì‹œ ì‚¬ìš©)
    train_images_dir = data_path / "images" / "train"
    train_images = []
    if train_images_dir.exists():
        train_images = sorted([f for f in train_images_dir.iterdir()
                              if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])

    # val ì´ë¯¸ì§€ë¥¼ ìš°ì„  ì‚¬ìš©, ë¶€ì¡±í•˜ë©´ trainì—ì„œ ë³´ì¶© (ìµœëŒ€ 100ê°œ)
    combined_images = val_images
    if len(combined_images) < 100 and train_images:
        needed = 100 - len(combined_images)
        combined_images = combined_images + train_images[:needed]
    
    # ìµœì¢…ì ìœ¼ë¡œ 100ê°œë¡œ ì œí•œ
    combined_images = combined_images[:100]

    if not combined_images:
        raise RuntimeError(f"No images found in {val_images_dir}")

    log_lines.append(
        f"ê²€ì¦ ì´ë¯¸ì§€: {len(combined_images)}ê°œ (ìµœëŒ€ 100ê°œë¡œ ì œí•œ) "
        f"(val: {min(len(val_images), 100)}ê°œ, train ì‚¬ìš©: {max(0, len(combined_images)-len(val_images))}ê°œ)"
    )
    
    # Ground-truth ë¡œë“œ
    ground_truth_obbs = []
    if val_labels_dir.exists():
        for label_file in val_labels_dir.glob("*.txt"):
            with open(label_file, 'r') as f:
                lines = f.readlines()
                ground_truth_obbs.extend(lines)
    
    # train labelsë„ í™•ì¸
    train_labels_dir = data_path / "labels" / "train"
    if train_labels_dir.exists() and len(combined_images) > len(val_images):
        for label_file in train_labels_dir.glob("*.txt"):
            with open(label_file, 'r') as f:
                lines = f.readlines()
                ground_truth_obbs.extend(lines)
    
    log_lines.append(f"Ground-truth OBB: {len(ground_truth_obbs)}ê°œ")
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(str(model_path))
    
    # Warm-up
    log_lines.append("Warm-up ì‹¤í–‰ ì¤‘...")
    _ = model.predict(
        source=str(val_images[0]),
        imgsz=spec["imgsz"],
        device=0,
        save=False,
        verbose=False,
    )
    synchronize()
    log_lines.append("Warm-up ì™„ë£Œ\n")
    
    # ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì •
    log_lines.append("ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    start_time = time.perf_counter()
    
    # ë°°ì¹˜ í¬ê¸° ì œí•œìœ¼ë¡œ ì¸í•´ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© ì²˜ë¦¬
    results_list = []
    total_detected_obbs = 0
    
    for img_path in combined_images:
        result = model.predict(
            source=str(img_path),
            imgsz=spec["imgsz"],
            device=0,
            save=False,
            verbose=False,
        )
        results_list.append(result[0])
        
        # ê²°ê³¼ ë¶„ì„
        if result[0].obb is not None:
            num_detections = len(result[0].obb)
            total_detected_obbs += num_detections
    
    synchronize()
    elapsed_time = time.perf_counter() - start_time
    
    # ê²°ê³¼ ë¶„ì„
    total_processed = len(results_list)
    
    avg_time_per_image = elapsed_time / total_processed if total_processed > 0 else 0
    
    # ê³µì‹ ì§€í‘œ ê³„ì‚° (YOLO val() ë©”ì„œë“œ ì‚¬ìš©)
    official_oriented_map = None
    try:
        log_lines.append("ê³µì‹ ì§€í‘œ ê³„ì‚° ì¤‘ (YOLO val() ë©”ì„œë“œ)...")
        # DOTAv1.5-1024_val ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (yamlì˜ pathë¥¼ í˜„ì¬ DATA_DIR ê¸°ì¤€ìœ¼ë¡œ ê°•ì œ ìˆ˜ì •)
        dataset_yaml = data_path / "dota1.5_yolo.yaml"
        if not dataset_yaml.exists():
            dataset_yaml = data_path / "DOTA1.5.yaml"
        if not dataset_yaml.exists():
            dataset_yaml = data_path / "dota.yaml"
        if not dataset_yaml.exists():
            dataset_yaml = None
        
        if dataset_yaml and dataset_yaml.exists():
            with open(dataset_yaml, "r", encoding="utf-8") as f:
                data_cfg = yaml.safe_load(f)
            if isinstance(data_cfg, dict):
                # yaml ì•ˆì˜ pathë¥¼ í˜„ì¬ data_pathë¡œ ê°•ì œ ì„¤ì •
                data_cfg["path"] = str(data_path)
                # val.cache íŒŒì¼ ì‚­ì œ (ì¬ìƒì„±ë˜ë„ë¡)
                val_cache = data_path / "labels" / "val.cache"
                if val_cache.exists():
                    val_cache.unlink()
                # ì„ì‹œ yaml íŒŒì¼ ìƒì„±
                temp_yaml = data_path / "temp_dota1.5_val.yaml"
                with open(temp_yaml, "w", encoding="utf-8") as f:
                    yaml.dump(data_cfg, f, default_flow_style=False, allow_unicode=True)
                try:
                    # YOLO val()ì€ ìë™ìœ¼ë¡œ ë¼ë²¨ì„ ì •ê·œí™”í•˜ì§€ë§Œ, DOTA í˜•ì‹ì´ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    # verbose=Trueë¡œ ì„¤ì •í•˜ì—¬ ë” ìì„¸í•œ ì—ëŸ¬ í™•ì¸
                    val_results = model.val(
                        data=str(temp_yaml),
                        imgsz=spec["imgsz"],
                        device=0,
                        verbose=False,
                        plots=False,
                    )
                except Exception as val_error:
                    log_lines.append(f"âš ï¸  val() ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {str(val_error)}")
                    # DOTA ë¼ë²¨ í˜•ì‹ì´ YOLO OBB í˜•ì‹ê³¼ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    log_lines.append("âš ï¸  DOTA ë¼ë²¨ í˜•ì‹(cx cy w h angle class_id difficulty)ì´ YOLO OBB í˜•ì‹ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    log_lines.append("âš ï¸  YOLO OBBëŠ” ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì‚¬ìš©í•˜ë©°, ë¼ë²¨ í˜•ì‹ì´ ë‹¤ë¥´ë©´ ê³µì‹ ì§€í‘œ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                    val_results = None
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    if temp_yaml.exists():
                        temp_yaml.unlink()
            else:
                log_lines.append("âš ï¸  yaml íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                val_results = None
            
            # oriented mAP@0.5:0.95
            if val_results is not None:
                try:
                    # val_resultsê°€ OBBMetrics íƒ€ì…ì¼ ìˆ˜ ìˆìŒ (obb ì†ì„±ì´ ì•„ë‹Œ ì§ì ‘)
                    if hasattr(val_results, "obb"):
                        obb_metrics = val_results.obb
                        if hasattr(obb_metrics, "map50_95"):
                            official_oriented_map = float(obb_metrics.map50_95)
                        elif hasattr(obb_metrics, "map"):
                            official_oriented_map = float(obb_metrics.map)
                        elif hasattr(obb_metrics, "maps"):
                            maps = obb_metrics.maps
                            if isinstance(maps, (list, tuple)) and len(maps) > 0:
                                official_oriented_map = float(maps[0])
                            elif hasattr(maps, "__len__") and len(maps) > 0:
                                # numpy ë°°ì—´ ë“±ì¸ ê²½ìš°
                                try:
                                    import numpy as np
                                    if isinstance(maps, np.ndarray):
                                        official_oriented_map = float(maps[0] if maps.size > 0 else 0.0)
                                    else:
                                        official_oriented_map = float(maps[0])
                                except (ImportError, (IndexError, TypeError)):
                                    # numpyê°€ ì—†ê±°ë‚˜ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ìš”ì†Œ ì‹œë„
                                    try:
                                        official_oriented_map = float(maps[0])
                                    except (IndexError, TypeError):
                                        pass
                            elif hasattr(maps, "__float__"):
                                try:
                                    official_oriented_map = float(maps)
                                except (TypeError, ValueError):
                                    # ë°°ì—´ì„ floatë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš°
                                    pass
                    # val_resultsê°€ ì§ì ‘ OBBMetrics ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
                    elif hasattr(val_results, "map50_95"):
                        official_oriented_map = float(val_results.map50_95)
                    elif hasattr(val_results, "map"):
                        official_oriented_map = float(val_results.map)
                    elif hasattr(val_results, "maps"):
                        maps = val_results.maps
                        if isinstance(maps, (list, tuple)) and len(maps) > 0:
                            official_oriented_map = float(maps[0])
                        elif hasattr(maps, "__len__") and len(maps) > 0:
                            # numpy ë°°ì—´ ë“±ì¸ ê²½ìš°
                            try:
                                import numpy as np
                                if isinstance(maps, np.ndarray):
                                    official_oriented_map = float(maps[0] if maps.size > 0 else 0.0)
                                else:
                                    official_oriented_map = float(maps[0])
                            except (ImportError, (IndexError, TypeError)):
                                # numpyê°€ ì—†ê±°ë‚˜ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ìš”ì†Œ ì‹œë„
                                try:
                                    official_oriented_map = float(maps[0])
                                except (IndexError, TypeError):
                                    pass
                        elif hasattr(maps, "__float__"):
                            try:
                                official_oriented_map = float(maps)
                            except (TypeError, ValueError):
                                # ë°°ì—´ì„ floatë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš°
                                pass
                    
                    if official_oriented_map is not None:
                        log_lines.append(f"ê³µì‹ ì§€í‘œ ê³„ì‚° ì™„ë£Œ: {official_oriented_map:.4f}")
                    else:
                        log_lines.append("âš ï¸  ê³µì‹ ì§€í‘œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                        log_lines.append(f"  val_results íƒ€ì…: {type(val_results)}")
                        # ëª¨ë“  ì†ì„± í™•ì¸ (private ì†ì„± ì œì™¸)
                        all_attrs = [attr for attr in dir(val_results) if not attr.startswith('_')]
                        log_lines.append(f"  val_results ì†ì„± (ì²˜ìŒ 20ê°œ): {all_attrs[:20]}")
                        # map ê´€ë ¨ ì†ì„± ì°¾ê¸°
                        map_attrs = [attr for attr in all_attrs if 'map' in attr.lower()]
                        if map_attrs:
                            log_lines.append(f"  map ê´€ë ¨ ì†ì„±: {map_attrs}")
                            # ì²« ë²ˆì§¸ map ì†ì„± ê°’ í™•ì¸
                            try:
                                first_map_attr = map_attrs[0]
                                map_value = getattr(val_results, first_map_attr)
                                log_lines.append(f"  {first_map_attr} ê°’: {map_value} (íƒ€ì…: {type(map_value)})")
                            except Exception as e:
                                log_lines.append(f"  {first_map_attr} ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")
                except Exception as extract_error:
                    log_lines.append(f"âš ï¸  ì§€í‘œ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {str(extract_error)}")
                    import traceback
                    log_lines.append(f"  ìƒì„¸: {traceback.format_exc()}")
            else:
                log_lines.append("âš ï¸  val() ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
        else:
            log_lines.append("âš ï¸  ë°ì´í„°ì…‹ yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê³µì‹ ì§€í‘œ ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        log_lines.append(f"âš ï¸  ê³µì‹ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
    
    # ë©”íŠ¸ë¦­ (OBBëŠ” ì •í™•í•œ ë§¤ì¹­ì´ ë³µì¡í•˜ë¯€ë¡œ íƒì§€ ìˆ˜ë§Œ ê¸°ë¡)
    metrics = {
        "total_detections": total_detected_obbs,
        "avg_detections_per_image": total_detected_obbs / total_processed if total_processed > 0 else 0,
    }
    if official_oriented_map is not None:
        metrics["official_oriented_map50_95"] = official_oriented_map
    
    log_lines.append(f"ì „ì²´ ì¶”ë¡  ì‹œê°„: {elapsed_time:.3f}ì´ˆ")
    log_lines.append(f"ì´ë¯¸ì§€ ìˆ˜: {total_processed}")
    log_lines.append(f"í‰ê·  ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {avg_time_per_image*1000:.3f}ms")
    log_lines.append(f"íƒì§€ëœ OBB: {total_detected_obbs}ê°œ")
    log_lines.append(f"ì´ë¯¸ì§€ë‹¹ í‰ê· : {metrics['avg_detections_per_image']:.2f}ê°œ")
    if official_oriented_map is not None:
        log_lines.append(f"[ê³µì‹ ì§€í‘œ] oriented mAP@[0.5:0.95]: {official_oriented_map:.4f}")
    log_lines.append("")
    
    return {
        "model": spec["name"],
        "task": spec["task"],
        "dataset": "DOTAv1.5-1024_val",
        "total_time": elapsed_time,
        "num_images": total_processed,
        "avg_time_per_image_ms": avg_time_per_image * 1000,
        "metrics": metrics,
    }


def validate_model(spec: Dict, log_lines: List[str]) -> Dict[str, Any]:
    """
    ëª¨ë¸ validation ìˆ˜í–‰ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    
    ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ì•¼ í•˜ë©°, ë³‘ë ¬ ì‹¤í–‰ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
    ê° ëª¨ë¸ì˜ validationì´ ì™„ì „íˆ ëë‚œ í›„ì—ë§Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    if spec["task"] == "classify":
        return validate_classify(spec, log_lines)
    elif spec["task"] == "obb":
        return validate_obb(spec, log_lines)
    else:
        # detect, pose, segment
        return validate_detect_seg_pose(spec, log_lines)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    RESULT_DIR.mkdir(parents=True, exist_ok=True)
    log_path = RESULT_DIR / "validation_alone.log"
    txt_path = RESULT_DIR / "validation_alone.txt"
    json_path = RESULT_DIR / "validation_alone.json"
    
    log_lines: List[str] = []
    summary_lines: List[str] = []
    summary_data: List[Dict] = []
    
    log_lines.append("="*80)
    log_lines.append("Validation ì‹œì‘")
    log_lines.append(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {RESULT_DIR}")
    log_lines.append("âš ï¸  ì¤‘ìš”: ê° engineì€ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë³‘ë ¬ ì‹¤í–‰ ì—†ìŒ)")
    log_lines.append("="*80)
    log_lines.append("")
    
    # ê° engineë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ validation ìˆ˜í–‰ (ë³‘ë ¬ ì‹¤í–‰ ì ˆëŒ€ ê¸ˆì§€)
    for idx, spec in enumerate(MODEL_SPECS, 1):
        try:
            print(f"\n[{idx}/{len(MODEL_SPECS)}] {spec['name']} validation ì‹œì‘ (ìˆœì°¨ ì‹¤í–‰)")
            summary = validate_model(spec, log_lines)
            summary_data.append(summary)
            print(f"[{idx}/{len(MODEL_SPECS)}] {spec['name']} validation ì™„ë£Œ")
            
            # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
            summary_lines.append(f"\n{'='*80}")
            summary_lines.append(f"[{summary['model']}]")
            summary_lines.append(f"  Dataset: {summary['dataset']}")
            summary_lines.append(f"  Task: {summary['task']}")
            summary_lines.append(f"  ì „ì²´ ì‹œê°„: {summary['total_time']:.3f}ì´ˆ")
            if summary['num_images'] > 0:
                summary_lines.append(f"  ì´ë¯¸ì§€ ìˆ˜: {summary['num_images']}")
                summary_lines.append(f"  í‰ê·  ì‹œê°„: {summary['avg_time_per_image_ms']:.3f}ms")
            
            # ë©”íŠ¸ë¦­ ì¶”ê°€
            metrics_str = format_metrics(summary['metrics'], summary['task'])
            if metrics_str:
                summary_lines.append("  ë©”íŠ¸ë¦­:")
                for line in metrics_str.split("\n"):
                    if line.strip():
                        summary_lines.append(line)
            
        except Exception as e:
            error_msg = f"[{spec['name']}] ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            log_lines.append(error_msg)
            log_lines.append("")
            summary_lines.append(error_msg)
            print(f"âŒ {error_msg}")
            import traceback
            log_lines.append(traceback.format_exc())
            log_lines.append("")
    
    # ê²°ê³¼ ì €ì¥
    log_lines.append("\n" + "="*80)
    log_lines.append("Validation ì™„ë£Œ")
    log_lines.append("="*80)
    
    log_path.write_text("\n".join(log_lines), encoding="utf-8")
    txt_path.write_text("\n".join(summary_lines), encoding="utf-8")
    
    # JSON í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (êµ¬ì¡°í™”ëœ ë°ì´í„°)
    json_path.write_text(
        json.dumps(summary_data, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )
    
    print(f"\nâœ… Validation ì™„ë£Œ!")
    print(f"ğŸ“„ ë¡œê·¸ íŒŒì¼: {log_path}")
    print(f"ğŸ“„ ìš”ì•½ íŒŒì¼: {txt_path}")
    print(f"ğŸ“„ JSON íŒŒì¼: {json_path}")


if __name__ == "__main__":
    main()
